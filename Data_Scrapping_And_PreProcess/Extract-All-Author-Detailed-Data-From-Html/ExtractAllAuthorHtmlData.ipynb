{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72091\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import sys\n",
    "import codecs\n",
    "\n",
    "# Get one html file from dir all-authors-html/ based on its appendix number, i.e., author_199.html\n",
    "# Extract all the search results:\n",
    "#     FirstName, LastName, \n",
    "#     NickName_1_FN, NickName_1_LN, \n",
    "#     NickName_2_FN, NickName_2_LN, \n",
    "#     NickName_3_FN, NickName_3_LN, \n",
    "#\n",
    "#     Subject\n",
    "#     Number_Of_Documents\n",
    "# \n",
    "#     Scopus_ID\n",
    "#     Affiliation(University)\n",
    "#     City\n",
    "#     Country\n",
    "#\n",
    "# Store result line-by-line into an csv file into dir authors_csv/ with naming convention: \n",
    "#     FirstName_LastName_HtmlFileNumber.csv\n",
    "\n",
    "authorId = []\n",
    "authorNm = []\n",
    "authorNickNames = []\n",
    "documents = []\n",
    "subjects = []\n",
    "affiliations = []\n",
    "city = []\n",
    "country = []\n",
    "\n",
    "authorUn = []\n",
    "\n",
    "\n",
    "\n",
    "def htmlToCsv(htmlnumber):\n",
    "    html_string = \"all-authors-html/author_\" + str(htmlnumber) + \".html\"\n",
    "    try:                         \n",
    "        html_doc = open(html_string,'r').read()\n",
    "        soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "        \n",
    "        st1 = soup.find(\"input\", {\"name\":\"st1\"}) \n",
    "        if st1 is not None:\n",
    "            st2 = soup.find(\"input\", {\"name\":\"st2\"}) # found firstname and lastname  \n",
    "            lastname = st1.get('value')\n",
    "            firstname = st2.get('value')\n",
    "            \n",
    "            filename = \"all-authors-csv-full-detail-data/\"+firstname + \"_\" + lastname + \"_\" + str(htmlnumber) + \".csv\"\n",
    "            target_file = open(filename, 'w')\n",
    "            \n",
    "            for a in soup.find_all('a'):\n",
    "                if a.get('title') == 'View this author\\'s profile':\n",
    "                    href = a.get('href')\n",
    "                    index_start = href.index('Id')\n",
    "                    index_end = href.index('&origin')\n",
    "                    authorId.append(href[index_start+3:index_end])\n",
    "                    authorNm.append(a.getText()[1:].replace(',',' '))\n",
    "            \n",
    "            for uni in soup.findAll(\"div\", { \"class\" : \"dataCol2\" }): # nicknames\n",
    "                try:\n",
    "                    authorNickNames.append(uni.getText())\n",
    "                except UnicodeEncodeError:\n",
    "                    authorNickNames.append(uni.getText().encode('ascii', 'ignore'))\n",
    "             \n",
    "            for uni in soup.findAll(\"div\", { \"class\" : \"dataCol3\" }): # number of documents\n",
    "                documents.append(uni.getText())\n",
    "            \n",
    "            for uni in soup.findAll(\"div\", { \"class\" : \"dataCol4\" }): # subject\n",
    "                subjects.append(uni.getText())\n",
    "                \n",
    "            for uni in soup.findAll(\"div\", { \"class\" : \"dataCol5\" }): # affiliation\n",
    "                try:\n",
    "                    affiliations.append(str(uni.getText())[1:-1])\n",
    "                except UnicodeEncodeError:\n",
    "                    affiliations.append(uni.getText().encode('ascii', 'ignore')[1:-1])\n",
    "                    \n",
    "            for uni in soup.findAll(\"div\", { \"class\" : \"dataCol6\" }): # city\n",
    "                try:\n",
    "                    city.append(str(uni.getText())[1:-1])\n",
    "                except UnicodeEncodeError:\n",
    "                    city.append(uni.getText().encode('ascii','ignore')[1:-1])\n",
    "                \n",
    "            for uni in soup.findAll(\"div\", { \"class\" : \"dataCol7\" }): # county\n",
    "                country.append(str(uni.getText())[1:-1])  \n",
    "                \n",
    "            authornames = []\n",
    "            for n in authorNickNames:\n",
    "                nicknames = []\n",
    "                try:\n",
    "                    names = str(n.strip()).splitlines()\n",
    "                except UnicodeEncodeError:\n",
    "                    names = n.strip().encode('ascii','ignore').splitlines()\n",
    "                for i in names:\n",
    "                    if i != '':\n",
    "                        nicknames.append(i.replace(',',' '))            \n",
    "                if len(nicknames) == 0:\n",
    "                    nicknames.append('')\n",
    "                    nicknames.append('')\n",
    "                    nicknames.append('')\n",
    "                if len(nicknames) == 1:\n",
    "                    nicknames.append('')\n",
    "                    nicknames.append('')\n",
    "                if len(nicknames) == 2:\n",
    "                    nicknames.append('')        \n",
    "                authornames.append(nicknames)\n",
    "            \n",
    "            numberOfDoc = []\n",
    "            for n in documents:\n",
    "                doc = str(n).splitlines()\n",
    "                for i in doc:\n",
    "                    if i!='' and i!='Documents':\n",
    "                        numberOfDoc.append(i)\n",
    "\n",
    "            subject_arr = []\n",
    "            for n in subjects:\n",
    "                sub = str(n).splitlines()\n",
    "                one_auth_sub = ''\n",
    "                \n",
    "                for idx, val in enumerate(sub):                                        \n",
    "                    if val != ';' and val != '':\n",
    "                        if idx == len(sub)-1:\n",
    "                            one_auth_sub += val.replace('; ...','')\n",
    "                        else:\n",
    "                            one_auth_sub += val + ' '\n",
    "                subject_arr.append(one_auth_sub)\n",
    "            \n",
    "            for i in xrange(0, len(authorId)):\n",
    "                authId = authorId[i]\n",
    "                try:\n",
    "                    authorName = str(authorNm[i])\n",
    "                except UnicodeEncodeError:\n",
    "                    authorName = authorNm[i].encode('ascii','ignore')\n",
    "                nicknames_arr = authornames[i]\n",
    "                numOfDoc = numberOfDoc[i]\n",
    "                subject_str = subject_arr[i].replace(',',' ')\n",
    "                aff = affiliations[i]\n",
    "                city_name = city[i]\n",
    "                count_name = country[i]\n",
    "                line = authId  + ',' + authorName + ',' + nicknames_arr[0] + ',' + nicknames_arr[1] + ',' +nicknames_arr[2] + ',' + numOfDoc + ',' + subject_str + ',' + aff + ',' + city_name + ',' + count_name\n",
    "                target_file.write(line.encode('utf-8'))\n",
    "                target_file.write('\\n')\n",
    "        else:\n",
    "            EmptyHtmlFiles_Log = open(\"ZERO_BYTE_HTML_LIST.txt\", \"a+\")\n",
    "            EmptyHtmlFiles_Log.write(\"author_\" + str(htmlnumber) + \".html has zero byte!\")\n",
    "            EmptyHtmlFiles_Log.write(\"\\n\")\n",
    "            EmptyHtmlFiles_Log.close()            \n",
    "    except IOError:\n",
    "        NoHtmlInputFile_Log = open(\"HTML_NOT_FOUND_LIST.txt\", \"a+\")\n",
    "        NoHtmlInputFile_Log.write(\"No input file: author_\" + str(htmlnumber) + \".html\")\n",
    "        NoHtmlInputFile_Log.write(\"\\n\")\n",
    "        NoHtmlInputFile_Log.close()\n",
    "    authorId[:] = []\n",
    "    authorNm[:] = []\n",
    "    authorNickNames[:] = []\n",
    "    documents[:] = []\n",
    "    subjects[:] = []\n",
    "    affiliations[:] = []\n",
    "    city[:] = []\n",
    "    country[:] = []\n",
    "    authorUn[:] = []\n",
    "    \n",
    "    \n",
    "for i in xrange(72091, 72092): #72091\n",
    "    htmlToCsv(i)\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
